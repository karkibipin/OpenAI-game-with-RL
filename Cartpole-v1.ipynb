{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using reinforcement learning to play OpenAI cartpole game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI was founded in late 2015 as a non-profit with a mission to “build safe artificial general intelligence (AGI) and ensure AGI’s benefits are as widely and evenly distributed as possible.” In addition to exploring many issues regarding AGI, one major contribution that OpenAI made to the machine learning world was developing both the Gym and Universe software platform. Gym is a collection of environments designed for testing and developing reinforcement learning algorithms. In this post we will train a neural network using reinforcement learning to play cartpole game in gym environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving forward let's discuss how reinforcement learning actually works? In a nutshell, RL is the study of agents and how they learn by trial and error. It formalizes the idea that rewarding or punishing an agent for its behavior makes it more likely to repeat or forego that behavior in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts and Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning, explained simply, is a computational approach where an agent interacts with an environment by taking actions in which it tries to maximize an accumulated reward. The image below shows the basic concept of RL. An agent in a current state $S_t$ takes an action $A_t$ to which the environment reacts and responds, returning a new state $S_{t+1}$ and reward $R{t+1}$ to the agent. Given the updated state and reward, the agent chooses the next action, and the loop repeats until an environment is solved or terminated.\n",
    "\n",
    "<img src=\"reinforcement-learning-fig1-700.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "##### Agent-environment interaction loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole from OpenAI Gym\n",
    "\n",
    "Before jumping forward let us understand the game first. The idea of CartPole is that there is a pole standing up on top of a cart. The goal is to balance this pole by wiggling or moving the cart from side to side to keep the pole balanced upright.\n",
    "\n",
    "The environment is deemed successful if we can balance the pole for 200 frames for cartpole-v0 and 500 frames for cartpole v1, and failure is deemed when the pole is more than 15 degrees from fully vertical or cart moves to the end of frame.\n",
    "\n",
    "Every frame that we go with the pole \"balanced\" (less than 15 degrees from vertical), our \"score\" gets +1, and our target is a score of 200 or 500. Here, we are using cartpole-v1 that make our target to 500.\n",
    "##### cartpole game\n",
    "<img src=\"SmartShortClownanemonefish-size_restricted.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to start by creating an agent. In the beginning, the agent  will just randomly chooses actions (left and right) when it is introduced in cartpole environment. Our goal is to get a score of 500 after training the model. Firstly, we will store the game information of any scenario where untrained model scores above 50 so that the agent can learn from. The input layer is the obervation from the environment, which includes pole position, cart position and such in an array. The output layer is actions: Left or Right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading required library\n",
    "import warnings\n",
    "with warnings.catch_warnings():  \n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    import gym\n",
    "    import numpy as np\n",
    "    import tflearn\n",
    "    from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "    from tflearn.layers.estimator import regression\n",
    "    from statistics import median, mean\n",
    "    from collections import Counter\n",
    "    import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02346698,  0.04203347, -0.04107528,  0.02749161])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setting up openai environment for cartpole game\n",
    "cartpole_env = gym.make(\"CartPole-v1\") #loading cartpole version 1\n",
    "cartpole_env.reset()  #resetting the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets move forward and play the game with an untrained agent and observe the output. Here output is the average score of all 100 games played by the agent with randomly selected actions 1 or 0 which is move leftor right. The agent is incapable of completing the game sice the actions are randomly taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untrained_random_game():\n",
    "    all_randomscores = []\n",
    "    \n",
    "#  Each episode is a single game\n",
    "#  Here machine plays 10 games\n",
    "\n",
    "    for episode in range(10):\n",
    "        cartpole_env.reset()\n",
    "        randomscores = 0\n",
    "        \n",
    "#  Here we are making single game to last for 100 frame but \n",
    "#  the random selection will not be able to play through out the game.\n",
    "        \n",
    "        for t in range(100):\n",
    "#  This will display the environment but it is time consuming\n",
    "            cartpole_env.render()\n",
    "            \n",
    "#  This will just create a sample action in any environment.\n",
    "#  In this environment, the action can be 0 or 1, which is left or right\n",
    "\n",
    "            action = cartpole_env.action_space.sample() # this line takes random action\n",
    "            \n",
    "#  This executes the environment with an randomly taken action (1 or 0), \n",
    "#  and returns the array of the observation of the environment, reward \n",
    "#  either 0 or 1, done (true or false) for game over, and other info.\n",
    "\n",
    "            observation, reward, done, info = cartpole_env.step(action)\n",
    "            randomscores+=reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        all_randomscores.append(randomscores)\n",
    "    average_score = sum(all_randomscores)/len(all_randomscores)    \n",
    "    print('100 Radom game play average score =', average_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Radom game play average score = 20.3\n"
     ]
    }
   ],
   "source": [
    "untrained_random_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting training data\n",
    "\n",
    "Each time when we observe the scene start over, that means the environment was \"done\" or game was over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = 70    #minimum score to add in training dataset\n",
    "number_of_games = 20000   #number of games to play for data collection\n",
    "\n",
    "# number of frames in each game, here action needs\n",
    "# to be taken 500 times in each game if succeed to play whole game\n",
    "\n",
    "highest_steps = 500 \n",
    "\n",
    "# This function is to collect data for training the model\n",
    "\n",
    "def training_data_collection():\n",
    "    \n",
    "# saves all observations and actions\n",
    "    training_data = []\n",
    "    \n",
    "# all scores\n",
    "    scores = []\n",
    "    \n",
    "# All the scores that met our threshold\n",
    "    accepted_scores = []\n",
    "    \n",
    "# iterate through number of games specified in number_of_games:\n",
    "\n",
    "    for _ in range(number_of_games):\n",
    "        score = 0 #score of individual game\n",
    "        \n",
    "# stores all the observations, info and action of individual game\n",
    "        game_memory = []\n",
    "\n",
    "# previous observation/array of cart position\n",
    "        prev_observation = []\n",
    "\n",
    "# for each frame in 500 (number of frames)\n",
    "        for _ in range(highest_steps):\n",
    "# choose random action (0 or 1)\n",
    "            action = random.randrange(0,2)\n",
    "# Let's play\n",
    "            observation, reward, done, info = cartpole_env.step(action)\n",
    "\n",
    "#  notice that the observation is returned FROM the action\n",
    "#  so we'll store the previous observation here, pairing\n",
    "#  the previous observation to the action we'll take.\n",
    "            if len(prev_observation) > 0 :\n",
    "                game_memory.append([prev_observation, action])\n",
    "            prev_observation = observation\n",
    "            score+=reward\n",
    "            if done: break\n",
    "\n",
    "# IF our score is higher than our threshold, we'd like to save\n",
    "# every move we made.\n",
    "# NOTE the reinforcement methodology here. \n",
    "# all we're doing is reinforcing the score, we're not trying \n",
    "# to influence the machine in any way as to HOW that score is \n",
    "# reached.\n",
    "\n",
    "        if score >= score_threshold:\n",
    "            accepted_scores.append(score)\n",
    "            for data in game_memory:\n",
    "                \n",
    "# convert to one-hot (this is the output layer for our neural network)\n",
    "                if data[1] == 1:\n",
    "                    output = [0,1]\n",
    "                elif data[1] == 0:\n",
    "                    output = [1,0]\n",
    "                    \n",
    "                # saving our training data\n",
    "                training_data.append([data[0], output])\n",
    "\n",
    "# reset cartpole_env to play again\n",
    "        cartpole_env.reset()\n",
    "    \n",
    "# save overall scores\n",
    "        scores.append(score)\n",
    "    \n",
    "# saving the training data for later use\n",
    "    training_data_save = np.array(training_data)\n",
    "    np.save('saved.npy',training_data_save)\n",
    "\n",
    "# printing basic stats of the scores\n",
    "    print('Average accepted score:',mean(accepted_scores))\n",
    "    print('Median score for accepted scores:',median(accepted_scores))\n",
    "    print(Counter(accepted_scores))\n",
    "    \n",
    "    return training_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a neural network architecture\n",
    "\n",
    "Building a neural network architecture with the use of relu and softmax activation function \n",
    "and adam is used as optimizer. We are using a simple multilayer perceptron model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3 #learning rate for the machine learning\n",
    "# learning rate is a number that we multiply our resulting gradient\n",
    "\n",
    "def neural_network_model(input_size):\n",
    "\n",
    "    network = input_data(shape=[None, input_size, 1], name='input')\n",
    "\n",
    "    network = fully_connected(network, 128, activation='relu')\n",
    "    network = dropout(network, 0.8)\n",
    "\n",
    "    network = fully_connected(network, 256, activation='relu')\n",
    "    network = dropout(network, 0.8)\n",
    "\n",
    "    network = fully_connected(network, 512, activation='relu')\n",
    "    network = dropout(network, 0.8)\n",
    "    \n",
    "    network = fully_connected(network, 512, activation='relu')\n",
    "    network = dropout(network, 0.8)\n",
    "        \n",
    "    network = fully_connected(network, 256, activation='relu')\n",
    "    network = dropout(network, 0.8)\n",
    "\n",
    "    network = fully_connected(network, 128, activation='relu')\n",
    "    network = dropout(network, 0.8)\n",
    "\n",
    "    network = fully_connected(network, 2, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
    "    model = tflearn.DNN(network, tensorboard_dir='log')\n",
    "\n",
    "    return model\n",
    "\n",
    "# concept of training model for optimizer adam (varient of sgd) is to minimize the loss between the \n",
    "# actual and the predictive output from given training samples\n",
    "\n",
    "def train_model(training_data, model=False):\n",
    "\n",
    "#observation of each frame as an input\n",
    "    X = np.array([i[0] for i in training_data]).reshape(-1,len(training_data[0][0]),1)\n",
    "    \n",
    "#action for each observation is the target\n",
    "    y = [i[1] for i in training_data] \n",
    "    \n",
    "    if not model:\n",
    "        model = neural_network_model(input_size = len(X[0]))\n",
    "    \n",
    "    model.fit({'input': X}, {'targets': y}, n_epoch=3, snapshot_step=500, \n",
    "              show_metric=True, run_id='cartpole-game')\n",
    "\n",
    "# The number of epochs is a hyperparameter that defines the number times that the \n",
    "# learning algorithm will work through the entire training dataset. One epoch means \n",
    "# that each sample in the training dataset has had an opportunity to update the internal model parameters.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(training_data, model=False):\n",
    "\n",
    "#observation of each frame as an input\n",
    "    X = np.array([i[0] for i in training_data]).reshape(-1,len(training_data[0][0]),1)\n",
    "    \n",
    "#action for each observation is the target\n",
    "    y = [i[1] for i in training_data] \n",
    "    \n",
    "    if not model:\n",
    "        model = neural_network_model(input_size = len(X[0]))\n",
    "    \n",
    "    model.fit({'input': X}, {'targets': y}, n_epoch=4, snapshot_step=500, \n",
    "              show_metric=True, run_id='cartpole-game')\n",
    "\n",
    "# The number of epochs is a hyperparameter that defines the number times that the \n",
    "# learning algorithm will work through the entire training dataset. One epoch means \n",
    "# that each sample in the training dataset has had an opportunity to update the internal model parameters.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 679  | total loss: \u001b[1m\u001b[32m0.64349\u001b[0m\u001b[0m | time: 1.704s\n",
      "| Adam | epoch: 004 | loss: 0.64349 - acc: 0.6212 -- iter: 10816/10833\n",
      "Training Step: 680  | total loss: \u001b[1m\u001b[32m0.64992\u001b[0m\u001b[0m | time: 1.714s\n",
      "| Adam | epoch: 004 | loss: 0.64992 - acc: 0.6153 -- iter: 10833/10833\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "training_data = training_data_collection()\n",
    "model = train_model(training_data)\n",
    "#save.model(\"cartpolev1\")  # This saves the model for later use and retrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play cartpole with trained model\n",
    "\n",
    "We are going to use our trained agent to play the game and save the obtained stats. The trained agent takes the action based on neural network model we trained to play the game. Instead of taking the random action the agent takes the action generated from the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: 499.44\n",
      "Minimum score 475.0\n",
      "Maximum score 500.0\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "choices = []\n",
    "for each_game in range(100):\n",
    "    score = 0\n",
    "    game_memory = []\n",
    "    prev_obs = []\n",
    "    cartpole_env.reset()\n",
    "    for _ in range(highest_steps):\n",
    "#         cartpole_env.render()    #this displays the game played by agent\n",
    "\n",
    "        if len(prev_obs)==0:\n",
    "            action = random.randrange(0,2)\n",
    "        else:\n",
    "            action = np.argmax(model.predict(prev_obs.reshape(-1,len(prev_obs),1))[0])\n",
    "\n",
    "        choices.append(action)\n",
    "                \n",
    "        new_observation, reward, done, info = cartpole_env.step(action)\n",
    "        prev_obs = new_observation\n",
    "        game_memory.append([new_observation, action])\n",
    "        score+=reward\n",
    "        if done: \n",
    "            break\n",
    "    scores.append(score)\n",
    "\n",
    "print('Average Score:',sum(scores)/len(scores))\n",
    "print(\"Minimum score\", min(scores))\n",
    "print(\"Maximum score\", max(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the difference between untrained agent and trained agent using reinforcement learning technique. To talk more specifically what RL does, we need to introduce additional terminology. We need to talk about\n",
    "\n",
    "1. states and observations,\n",
    "2. action spaces,\n",
    "3. policies,\n",
    "4. trajectories,\n",
    "5. different formulations of return,\n",
    "6. the RL optimization problem,\n",
    "7. and value functions.\n",
    "\n",
    "These terms are explained in details in OpenAI spinning up in given link\n",
    "https://spinningup.openai.com/en/latest/spinningup/rl_intro.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
